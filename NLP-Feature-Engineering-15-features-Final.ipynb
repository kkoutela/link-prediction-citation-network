{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb4032e3",
   "metadata": {},
   "source": [
    "## Feature Engineering to train our models  \n",
    "\n",
    "<b> End up in 15 Features: </b>\n",
    "\n",
    "1) Number of overlapping words in title  \n",
    "2) Temporal distance between the papers  \n",
    "3) Number of common authors  \n",
    "4) Number of overlapping words in journal  \n",
    "5) Number of overlapping words in abstract  \n",
    "6) Cosine similarity of abstract  \n",
    "7) Cosine similarity of title  \n",
    "8) Cosine similarity of author  \n",
    "9) Cosine similarity of journal\n",
    "\n",
    "<b> Graph Based Features</b>\n",
    "\n",
    "10) Jaccard similarity coefficient  \n",
    "11) Preferential attachment score  \n",
    "12) Adamic Adar Index  \n",
    "13) Common neighbours  \n",
    "14) Same Cluster (community)  \n",
    "15) Community resource allocation  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c56cf1",
   "metadata": {},
   "source": [
    "### Import the appropriate packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab8317f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "205265a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for networks\n",
    "\n",
    "import networkx as nx\n",
    "from networkx import *\n",
    "#!pip install python-louvain\n",
    "import community.community_louvain as community_louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e945d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91b3a97",
   "metadata": {},
   "source": [
    "### Open the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c98f54ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Testing Set\n",
    "with open(\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3115ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Set\n",
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47a05d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Node Set\n",
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbad8b4",
   "metadata": {},
   "source": [
    "### Preprossesing for consine Similarity (Abstract , Title ,  Author & Journal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e38e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute TFIDF vector of each paper Abstract\n",
    "corpus = [element[5] for element in node_info] #get the abstract from node info file\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words=\"english\", max_features=50000) #we add the combination of two words as well, but since the output will be quite large we want to minimize to the 50.000 double of the inital\n",
    "features_TFIDF_abstr = vectorizer.fit_transform(corpus)\n",
    "\n",
    "#features_TFIDF_abstr.shape -> to check the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f116a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute TFIDF vector of each paper Title\n",
    "corpus = [element[2] for element in node_info]  #get the title from node info file\n",
    "vectorizer = TfidfVectorizer (ngram_range=(1, 2), stop_words=\"english\", max_features=10000) #we add the combination of two words as well, but since the output will be quite large we want to minimize to the 10.0000 almostdouble of the inital\n",
    "features_TFIDF_title = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a13b0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute TFIDF vector of each author\n",
    "corpus = [element[3] for element in node_info]\n",
    "vectorizer = TfidfVectorizer (ngram_range=(1, 2), stop_words=\"english\", max_features=20000) #we add the combination of two words as well, but since the output will be quite large we want to minimize to the 20.000 almost double of the inital\n",
    "features_TFIDF_author = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77eef59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute TFIDF vector of each journal\n",
    "corpus = [element[4] for element in node_info]\n",
    "vectorizer = TfidfVectorizer (ngram_range=(1, 2), stop_words=\"english\") \n",
    "features_TFIDF_journal = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c91c6c1",
   "metadata": {},
   "source": [
    "### Preprossesing for Features from Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1031a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = [(element[0], element[1]) for element in training_set if element[2] == \"1\"]\n",
    "# some nodes may not be connected to any other node\n",
    "# hence the need to create the nodes of the graph from node_info.csv,\n",
    "# not just from the edge list\n",
    "nodes = IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c59d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the graph with the networkX library\n",
    "gx = nx.Graph() \n",
    "gx.add_nodes_from(nodes)\n",
    "gx.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "004927dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find communities\n",
    "\n",
    "partition = community_louvain.best_partition(gx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93945e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign communities at each node\n",
    "for i in nodes:\n",
    "    gx.nodes[i][\"community\"] = partition[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "461bc00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check if they belong in the same community (if so, outpout 1)\n",
    "def in_same_cluster(s, d, partition):\n",
    "    if partition[s] ==  partition[d]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc38ce",
   "metadata": {},
   "source": [
    "# Initialise the process of Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b11294c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for each training example we need to compute features  \n",
    "# # in this baseline we will train the model on only 1% of the training set  \n",
    "\n",
    "# to_keep = random.sample(range(len(training_set)), k=int(round(len(training_set)*0.01)))\n",
    "# training_set_reduced = [training_set[i] for i in to_keep]\n",
    "\n",
    "training_set_reduced= training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d147429f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "10001 training examples processsed\n",
      "20001 training examples processsed\n",
      "30001 training examples processsed\n",
      "40001 training examples processsed\n",
      "50001 training examples processsed\n",
      "60001 training examples processsed\n",
      "70001 training examples processsed\n",
      "80001 training examples processsed\n",
      "90001 training examples processsed\n",
      "100001 training examples processsed\n",
      "110001 training examples processsed\n",
      "120001 training examples processsed\n",
      "130001 training examples processsed\n",
      "140001 training examples processsed\n",
      "150001 training examples processsed\n",
      "160001 training examples processsed\n",
      "170001 training examples processsed\n",
      "180001 training examples processsed\n",
      "190001 training examples processsed\n",
      "200001 training examples processsed\n",
      "210001 training examples processsed\n",
      "220001 training examples processsed\n",
      "230001 training examples processsed\n",
      "240001 training examples processsed\n",
      "250001 training examples processsed\n",
      "260001 training examples processsed\n",
      "270001 training examples processsed\n",
      "280001 training examples processsed\n",
      "290001 training examples processsed\n",
      "300001 training examples processsed\n",
      "310001 training examples processsed\n",
      "320001 training examples processsed\n",
      "330001 training examples processsed\n",
      "340001 training examples processsed\n",
      "350001 training examples processsed\n",
      "360001 training examples processsed\n",
      "370001 training examples processsed\n",
      "380001 training examples processsed\n",
      "390001 training examples processsed\n",
      "400001 training examples processsed\n",
      "410001 training examples processsed\n",
      "420001 training examples processsed\n",
      "430001 training examples processsed\n",
      "440001 training examples processsed\n",
      "450001 training examples processsed\n",
      "460001 training examples processsed\n",
      "470001 training examples processsed\n",
      "480001 training examples processsed\n",
      "490001 training examples processsed\n",
      "500001 training examples processsed\n",
      "510001 training examples processsed\n",
      "520001 training examples processsed\n",
      "530001 training examples processsed\n",
      "540001 training examples processsed\n",
      "550001 training examples processsed\n",
      "560001 training examples processsed\n",
      "570001 training examples processsed\n",
      "580001 training examples processsed\n",
      "590001 training examples processsed\n",
      "600001 training examples processsed\n",
      "610001 training examples processsed\n",
      "End of Process\n",
      "Wall time: 1h 39min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initilaise the required lists for our features\n",
    "\n",
    "#1: Number of overlapping words in title\n",
    "overlap_title = []\n",
    "#2: Temporal distance between the papers\n",
    "temp_diff = []\n",
    "#3: Number of common authors\n",
    "comm_auth = []\n",
    "#4: Number of overlapping words in journal\n",
    "overlap_journ = []\n",
    "#5: Number of overlapping words in adstract\n",
    "overlap_abstr = []\n",
    "#6: Cosine similarity of abstract\n",
    "cos_sim_abstr = []\n",
    "#7: Cosine similarity title\n",
    "cos_sim_title = []\n",
    "#8: Cosine similarity author\n",
    "cos_sim_author = []\n",
    "#9: Cosine similarity journal\n",
    "cos_sim_journal = []\n",
    "#10: Jaccard similarity coefficient\n",
    "jac_sim = []\n",
    "#11: Preferential attachment score\n",
    "pref_attac = []\n",
    "#12: Adamic Adar Index\n",
    "adam_index = []\n",
    "#13: Common neighbors\n",
    "com_neigh = []\n",
    "#14: Same Cluster (community)\n",
    "same_cluster=[]\n",
    "#15: Community resource allocation\n",
    "commun_ra = []\n",
    "# Extra:  Common Neighbor and Centrality based Parameterized Algorithm(CCPA) - It was not running, maybe computational expensive \n",
    "\n",
    "\n",
    "counter = 0\n",
    "for i in range(len(training_set_reduced)):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    ### Preprocessing of data\n",
    "    #1 -> Title (source and targe) convert to lowercase, tokenize, remove stopwords and stemming\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "   \n",
    "    #2 -> Manipulate authors (source and targe) \n",
    "    source_auth = source_info[3]\n",
    "    source_auth= re.sub(r'\\([^\\)]*(?=\\.\\w+$)', '', source_auth) #in some cases there were also the university denoted in parenthesis -> first close parentheses\n",
    "    source_auth = re.sub(r'\\([^()]*\\)', '', source_auth)  #then remove it\n",
    "    source_auth = source_auth.split(\",\") \n",
    "    \n",
    "    target_auth = target_info[3]\n",
    "    target_auth= re.sub(r'\\([^\\)]*(?=\\.\\w+$)', '', target_auth) #in some cases there were also the university denoted in parenthesis -> first close parentheses\n",
    "    target_auth = re.sub(r'\\([^()]*\\)', '', target_auth)  #then remove it\n",
    "    target_auth = target_auth.split(\",\") \n",
    "    \n",
    "    \n",
    "    #3-> Same process as Title for Abstract\n",
    "    source_abstr = source_info[5].lower().split(\" \")  \n",
    "    source_abstr = [token for token in source_abstr if token not in stpwds] \n",
    "    source_abstr = [stemmer.stem(token) for token in source_abstr]  \n",
    "    \n",
    "    target_abstr = target_info[5].lower().split(\" \")  # convert to lowercase and tokenize\n",
    "    target_abstr = [token for token in target_abstr if token not in stpwds]  # remove stopwords\n",
    "    target_abstr = [stemmer.stem(token) for token in target_abstr]  # perform stemming\n",
    "\n",
    "    \n",
    "    #4-> Call Journal(source and targe) \n",
    "    source_journal = source_info[4]\n",
    "    target_journal = target_info[4]\n",
    "\n",
    "    \n",
    "    #Features extraction \n",
    "    #1 ->  Overlapping words in Title\n",
    "    overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "    #2 -> Difference of Publication Year\n",
    "    temp_diff.append(int(source_info[1]) - int(target_info[1]))\n",
    "    #3 -> Common Authors\n",
    "    comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "    #4 - Overlapping words in Journal\n",
    "    overlap_journ.append(len(set(source_journal).intersection(set(target_journal))))\n",
    "    #5 - Overlapping Abstract words\n",
    "    overlap_abstr.append(len(set(source_abstr).intersection(set(target_abstr))))\n",
    "    #6 - Cosine similarity of Abstract\n",
    "    cos_sim_abstr.append(cosine_similarity(features_TFIDF_abstr[index_source], features_TFIDF_abstr[index_target]))\n",
    "    #7 - Cosine similarity of Title\n",
    "    cos_sim_title.append(cosine_similarity(features_TFIDF_title[index_source], features_TFIDF_title[index_target]))\n",
    "    #8 - Cosine similarity of Author\n",
    "    cos_sim_author.append(cosine_similarity(features_TFIDF_author[index_source], features_TFIDF_author[index_target]))\n",
    "    #9 - Cosine similarity of Journal\n",
    "    cos_sim_journal.append(cosine_similarity(features_TFIDF_journal[index_source], features_TFIDF_journal[index_target]))\n",
    "    \n",
    "    # From Graphs\n",
    "    #10 Jaccard similarity coeffieent\n",
    "    pred = nx.jaccard_coefficient(gx,[(str(source), str(target))]) #string since only this way will be able to handle \n",
    "    for u, v ,p in pred:\n",
    "        jac_sim.append(p)\n",
    "        \n",
    "    #11: Preferential attachment score\n",
    "    pref = nx.preferential_attachment(gx,[(str(source), str(target))]) #string since only this way will be able to handle\n",
    "    for u, v ,p in pref:\n",
    "        pref_attac.append(p)\n",
    "    \n",
    "    #12: Adamic Adar Index\n",
    "    adam = nx.adamic_adar_index(gx,[(str(source), str(target))]) #string since only this way will be able to handle\n",
    "    for u, v ,p in adam:\n",
    "        adam_index.append(p)\n",
    "        \n",
    "    #13: common neighbors\n",
    "    com_neigh.append(len(sorted(nx.common_neighbors(gx, str(source), str(target)))))\n",
    "    \n",
    "    #14: same cluster\n",
    "    same_cluster.append(in_same_cluster(str(source), str(target), partition))\n",
    "    \n",
    "    #15: Community resource allocation\n",
    "    cra = nx.ra_index_soundarajan_hopcroft(gx,[(str(source), str(target))]) #same as 8\n",
    "    for u, v ,p in cra:\n",
    "        commun_ra.append(p)\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print (counter, \"training examples processsed\")\n",
    "        \n",
    "print (\"End of Process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a993e224",
   "metadata": {},
   "source": [
    "### Insert 15 new features in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2816177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_11432/4264973818.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training_features = np.array([overlap_title, temp_diff, comm_auth, overlap_journ, overlap_abstr, cos_sim_abstr, cos_sim_title, cos_sim_author, cos_sim_journal, jac_sim, pref_attac, adam_index, com_neigh, same_cluster, commun_ra]).T\n"
     ]
    }
   ],
   "source": [
    "# convert list of lists into array\n",
    "# documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "training_features = np.array([overlap_title, temp_diff, comm_auth, overlap_journ, overlap_abstr, cos_sim_abstr, cos_sim_title, cos_sim_author, cos_sim_journal, jac_sim, pref_attac, adam_index, com_neigh, same_cluster, commun_ra]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ffe73bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale them before save\n",
    "training_features = preprocessing.scale(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e95212a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_scaled = zip(range(len(training_set)), training_features)\n",
    "# header = ['overlap_title', 'temp_diff', 'comm_auth', 'overlap_journ', 'overlap_abstr', 'cos_sim_abstr', 'cos_sim_title', 'cos_sim_author', 'cos_sim_journal', 'jac_sim', 'pref_attac', 'adam_index', 'com_neigh', 'same_cluster', 'commun_ra']\n",
    "with open(\"training_features_scaled.csv\",\"w\") as tr_feat:\n",
    "    csv_out = csv.writer(tr_feat)\n",
    "#     csv_out.writerow(header)\n",
    "    for row in training_features:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529056c1",
   "metadata": {},
   "source": [
    "#### Add also labels in a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27b381be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels into integers then into column array\n",
    "labels = [int(element[2]) for element in training_set_reduced]\n",
    "labels = list(labels)\n",
    "labels_array = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cc35308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import savetxt\n",
    "# save to csv file\n",
    "savetxt('labels.csv', labels_array, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e8507",
   "metadata": {},
   "source": [
    "## Same process for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f4c0f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 testing examples processsed\n",
      "10001 testing examples processsed\n",
      "20001 testing examples processsed\n",
      "30001 testing examples processsed\n",
      "End of Process\n",
      "Wall time: 7min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test -> same process as train\n",
    "# we need to compute the features for the testing set\n",
    "\n",
    "overlap_title_test = []\n",
    "temp_diff_test = []\n",
    "comm_auth_test = []\n",
    "overlap_journ_test = []\n",
    "overlap_abstr_test = []\n",
    "cos_sim_abstr_test = []\n",
    "cos_sim_title_test = []\n",
    "cos_sim_author_test = []\n",
    "cos_sim_journal_test = []\n",
    "jac_sim_test = []\n",
    "pref_attac_test = []\n",
    "adam_index_test = []\n",
    "com_neigh_test = []\n",
    "same_cluster_test=[]\n",
    "commun_ra_test = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for i in range(len(testing_set)):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "   ### Preprocessing of data\n",
    "    #1 -> Title (source and targe) convert to lowercase, tokenize, remove stopwords and stemming\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "   \n",
    "    #2 -> Manipulate authors (source and targe) \n",
    "    source_auth = source_info[3]\n",
    "    source_auth= re.sub(r'\\([^\\)]*(?=\\.\\w+$)', '', source_auth) #in some cases there were also the university denoted in parenthesis -> first close parentheses\n",
    "    source_auth = re.sub(r'\\([^()]*\\)', '', source_auth)  #then remove it\n",
    "    source_auth = source_auth.split(\",\") \n",
    "    \n",
    "    target_auth = target_info[3]\n",
    "    target_auth= re.sub(r'\\([^\\)]*(?=\\.\\w+$)', '', target_auth) #in some cases there were also the university denoted in parenthesis -> first close parentheses\n",
    "    target_auth = re.sub(r'\\([^()]*\\)', '', target_auth)  #then remove it\n",
    "    target_auth = target_auth.split(\",\") \n",
    "    \n",
    "    #3-> Same process as Title for Abstract\n",
    "    source_abstr = source_info[5].lower().split(\" \")  \n",
    "    source_abstr = [token for token in source_abstr if token not in stpwds] \n",
    "    source_abstr = [stemmer.stem(token) for token in source_abstr]  \n",
    "    \n",
    "    target_abstr = target_info[5].lower().split(\" \")  # convert to lowercase and tokenize\n",
    "    target_abstr = [token for token in target_abstr if token not in stpwds]  # remove stopwords\n",
    "    target_abstr = [stemmer.stem(token) for token in target_abstr]  # perform stemming\n",
    "\n",
    "    #4-> Call Journal(source and targe) \n",
    "    source_journal = source_info[4]\n",
    "    target_journal = target_info[4]\n",
    "\n",
    "    \n",
    "    #Features extraction \n",
    "    overlap_title_test.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff_test.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth_test.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "    overlap_journ_test.append(len(set(source_journal).intersection(set(target_journal))))\n",
    "    overlap_abstr_test.append(len(set(source_abstr).intersection(set(target_abstr))))\n",
    "    cos_sim_abstr_test.append(cosine_similarity(features_TFIDF_abstr[index_source], features_TFIDF_abstr[index_target]))\n",
    "    cos_sim_title_test.append(cosine_similarity(features_TFIDF_title[index_source], features_TFIDF_title[index_target]))\n",
    "    cos_sim_author_test.append(cosine_similarity(features_TFIDF_author[index_source], features_TFIDF_author[index_target]))\n",
    "    cos_sim_journal_test.append(cosine_similarity(features_TFIDF_journal[index_source], features_TFIDF_journal[index_target]))\n",
    "   \n",
    "    \n",
    "    #Graph Features\n",
    "    #1\n",
    "    pred = nx.jaccard_coefficient(gx,[(str(source), str(target))]) #string since only this way will be able to handle \n",
    "    for u, v ,p in pred:\n",
    "        jac_sim_test.append(p)\n",
    "        \n",
    "    #2\n",
    "    pref = nx.preferential_attachment(gx,[(str(source), str(target))]) #string since only this way will be able to handle\n",
    "    for u, v ,p in pref:\n",
    "        pref_attac_test.append(p)\n",
    "    \n",
    "    #3\n",
    "    adam = nx.adamic_adar_index(gx,[(str(source), str(target))]) #string since only this way will be able to handle\n",
    "    for u, v ,p in adam:\n",
    "        adam_index_test.append(p)\n",
    "        \n",
    "    #4\n",
    "    com_neigh_test.append(len(sorted(nx.common_neighbors(gx, str(source), str(target)))))\n",
    "    \n",
    "    #5\n",
    "    same_cluster_test.append(in_same_cluster(str(source), str(target), partition))\n",
    "    \n",
    "    #6\n",
    "    cra_t = nx.ra_index_soundarajan_hopcroft(gx,[(str(source), str(target))]) #same as 8\n",
    "    for u, v ,p in cra_t:\n",
    "        commun_ra_test.append(p)\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print (counter, \"testing examples processsed\")\n",
    "        \n",
    "print (\"End of Process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2700edad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_11432/2274836015.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  testing_features = np.array([overlap_title_test,temp_diff_test,comm_auth_test, overlap_journ_test,overlap_abstr_test,cos_sim_abstr_test, cos_sim_title_test, cos_sim_author_test, cos_sim_journal_test,jac_sim_test, pref_attac_test, adam_index_test,com_neigh_test, same_cluster_test,commun_ra_test]).T\n"
     ]
    }
   ],
   "source": [
    "# convert list of lists into array\n",
    "# documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "testing_features = np.array([overlap_title_test,temp_diff_test,comm_auth_test, overlap_journ_test,overlap_abstr_test,cos_sim_abstr_test, cos_sim_title_test, cos_sim_author_test, cos_sim_journal_test,jac_sim_test, pref_attac_test, adam_index_test,com_neigh_test, same_cluster_test,commun_ra_test]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b35f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale\n",
    "testing_features = preprocessing.scale(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ac2f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_features_scaled = zip(range(len(testing_set)), testing_features)\n",
    "header=['overlap_title_test','temp_diff_test','comm_auth_test', 'overlap_journ_test','overlap_abstr_test','cos_sim_abstr_test', 'cos_sim_title_test', 'cos_sim_author_test', 'cos_sim_journal_test','jac_sim_test', 'pref_attac_test', 'adam_index_test','com_neigh_test', 'same_cluster_test','commun_ra_test']\n",
    "with open(\"testing_features_scaled.csv\",\"w\") as test_feat:\n",
    "    csv_out = csv.writer(test_feat)\n",
    "#     csv_out.writerow(header)\n",
    "    for row in testing_features:\n",
    "        csv_out.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
